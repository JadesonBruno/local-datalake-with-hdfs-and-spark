# ğŸ—ï¸ Local Data Lake with HDFS and Spark

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Docker](https://img.shields.io/badge/Docker-20.10+-blue.svg)](https://www.docker.com/)
[![Spark](https://img.shields.io/badge/Apache%20Spark-3.5.1-E25A1C.svg)](https://spark.apache.org/)
[![Hadoop](https://img.shields.io/badge/Apache%20Hadoop-3.4.0-yellow.svg)](https://hadoop.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)

A production-ready local Data Lake implementation using Apache Hadoop HDFS and Apache Spark for distributed data processing and machine learning workloads.

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Architecture](#-architecture)
- [Project Structure](#-project-structure)
- [Use Case](#-use-case)
- [Prerequisites](#-prerequisites)
- [Quick Start](#-quick-start)
- [Web Interfaces](#-web-interfaces)
- [Running the ML Job](#-running-the-ml-job)
- [Model Results](#-model-results)
- [HDFS Operations](#-hdfs-operations)
- [Cluster Management](#-cluster-management)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

## ğŸ¯ Overview

This project demonstrates a complete Data Lake architecture running locally using Docker containers. It implements:

- **Distributed Storage**: HDFS (Hadoop Distributed File System) for reliable, scalable data storage
- **Distributed Processing**: Apache Spark with YARN resource manager for parallel data processing
- **Machine Learning Pipeline**: PySpark ML for training and evaluating classification models
- **Monitoring**: Spark History Server for job tracking and performance analysis

### Key Features

- ğŸ³ Fully containerized with Docker Compose
- ğŸ“Š Complete ML pipeline with cross-validation
- ğŸ”„ Multi-node cluster (1 master + N workers)
- ğŸ“ˆ Real-time monitoring via web UIs
- ğŸ’¾ Persistent data storage with HDFS
- ğŸš€ Production-ready configuration

## ğŸ›ï¸ Architecture

```mermaid
graph TB
    subgraph "Docker Network"
        subgraph "Master Node"
            Master[Master Container]
            NN[HDFS NameNode]
            RM[YARN ResourceManager]
            SNN[Secondary NameNode]
        end
        
        subgraph "Worker Nodes"
            W1[Worker 1]
            W2[Worker 2]
            W3[Worker N]
            DN1[DataNode 1]
            DN2[DataNode 2]
            DN3[DataNode N]
            NM1[NodeManager 1]
            NM2[NodeManager 2]
            NM3[NodeManager N]
        end
        
        subgraph "History Server"
            HS[History Server Container]
            HSP[Spark History Process]
        end
        
        subgraph "Shared Storage"
            HDFS[(HDFS Storage)]
            Logs[(Event Logs Volume)]
        end
    end
    
    Client[Client/User] -->|Submit Job| Master
    Master -->|Manage Resources| RM
    RM -->|Allocate| NM1
    RM -->|Allocate| NM2
    RM -->|Allocate| NM3
    NN -->|Replicate| DN1
    NN -->|Replicate| DN2
    NN -->|Replicate| DN3
    DN1 --> HDFS
    DN2 --> HDFS
    DN3 --> HDFS
    Master -.->|Read/Write| HDFS
    W1 -.->|Read/Write| HDFS
    W2 -.->|Read/Write| HDFS
    W3 -.->|Read/Write| HDFS
    Master -->|Write Events| Logs
    W1 -->|Write Events| Logs
    W2 -->|Write Events| Logs
    W3 -->|Write Events| Logs
    HS -->|Read Events| Logs
    Client -.->|Monitor| Master
    Client -.->|View History| HS
    
    style Master fill:#e1f5ff
    style W1 fill:#fff3e0
    style W2 fill:#fff3e0
    style W3 fill:#fff3e0
    style HS fill:#f3e5f5
    style HDFS fill:#c8e6c9
    style Logs fill:#c8e6c9
```

### Components

| Component | Role | Port |
|-----------|------|------|
| **Master Node** | Coordinates cluster, runs NameNode & ResourceManager | 9870 (HDFS), 8088 (YARN) |
| **Worker Nodes** | Execute tasks, run DataNodes & NodeManagers | Dynamic |
| **History Server** | Tracks completed Spark jobs | 18080 |
| **HDFS** | Distributed file system for data storage | 9000 |
| **YARN** | Resource manager and job scheduler | 8088 |

## ğŸ“ Project Structure

```
local-datalake-with-hdfs-and-spark/
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml          # Multi-container orchestration
â”œâ”€â”€ ğŸ“„ Dockerfile                  # Spark + Hadoop image definition
â”œâ”€â”€ ğŸ“„ entrypoint.sh              # Container initialization script
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ“„ .env                        # Environment variables (not tracked)
â”œâ”€â”€ ğŸ“„ .gitignore                 # Git ignore rules
â”œâ”€â”€ ğŸ“„ LICENSE                    # Project license
â”œâ”€â”€ ğŸ“„ README.md                  # This file
â”‚
â”œâ”€â”€ ğŸ“‚ data/                       # Local data & HDFS outputs
â”‚   â”œâ”€â”€ dataset.csv               # Input: Census income dataset
â”‚   â”œâ”€â”€ part-*.csv                # Output: Processed predictions
â”‚   â””â”€â”€ model/                    # Trained model artifacts
â”‚       â”œâ”€â”€ bestModel/            # Best CV model
â”‚       â”‚   â”œâ”€â”€ data/
â”‚       â”‚   â”‚   â””â”€â”€ _SUCCESS
â”‚       â”‚   â””â”€â”€ metadata/
â”‚       â”‚       â”œâ”€â”€ _SUCCESS
â”‚       â”‚       â””â”€â”€ part-00000
â”‚       â”œâ”€â”€ estimator/            # LogisticRegression config
â”‚       â”‚   â””â”€â”€ metadata/
â”‚       â”œâ”€â”€ evaluator/            # BinaryClassificationEvaluator config
â”‚       â”‚   â””â”€â”€ metadata/
â”‚       â””â”€â”€ metadata/             # CrossValidator metadata
â”‚           â”œâ”€â”€ _SUCCESS
â”‚           â””â”€â”€ part-00000        # Contains avgMetrics & best params
â”‚
â”œâ”€â”€ ğŸ“‚ jobs/                       # PySpark application code
â”‚   â””â”€â”€ job.py                    # ML pipeline: preprocessing + training + evaluation
â”‚
â”œâ”€â”€ ğŸ“‚ logs/                       # Container logs (generated at runtime)
â”‚
â”œâ”€â”€ ğŸ“‚ ssh/                        # SSH configuration for Hadoop
â”‚   â””â”€â”€ ssh_config                # Passwordless SSH setup
â”‚
â””â”€â”€ ğŸ“‚ yarn/                       # Hadoop & Spark configuration files
    â”œâ”€â”€ capacity-scheduler.xml    # YARN capacity scheduler config
    â”œâ”€â”€ core-site.xml             # Hadoop core settings (HDFS URI)
    â”œâ”€â”€ hdfs-site.xml             # HDFS replication & storage
    â”œâ”€â”€ mapred-site.xml           # MapReduce framework settings
    â”œâ”€â”€ spark-defaults.conf       # Spark default configuration
    â””â”€â”€ yarn-site.xml             # YARN resource management
```

## ğŸ“ Use Case

### Problem Statement

Predict whether a person's annual income exceeds $50,000 based on census data (age, education, occupation, etc.).

### Dataset

**Source**: UCI Adult Census Income Dataset  
**Records**: ~32,561 individuals  
**Features**: 14 attributes (8 categorical, 6 numerical)  
**Target**: Binary classification (`>50K` or `<=50K`)

### ML Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Data Preprocessing                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Load CSV from HDFS                                              â”‚
â”‚  2. Handle missing values (fill "Unknown" for categoricals)         â”‚
â”‚  3. Drop remaining NAs                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Feature Engineering                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  4. StringIndexer â†’ Convert categorical strings to indices          â”‚
â”‚  5. OneHotEncoder â†’ Encode categorical indices to vectors           â”‚
â”‚  6. VectorAssembler â†’ Combine all features into single vector       â”‚
â”‚  7. StringIndexer â†’ Convert target "income" to binary label         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Model Training & Tuning                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  8. Split data â†’ 70% train, 30% test (seed=42)                     â”‚
â”‚  9. LogisticRegression estimator                                    â”‚
â”‚ 10. ParamGrid â†’ regParam [0.1, 0.01]                               â”‚
â”‚ 11. CrossValidator â†’ 5-fold CV, AUC metric, 3 parallel tasks       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Model Evaluation & Export                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 12. Evaluate on test set â†’ Compute AUC-ROC                         â”‚
â”‚ 13. Save best model to HDFS â†’ hdfs:///opt/spark/data/model         â”‚
â”‚ 14. Save AUC metric to HDFS â†’ hdfs:///opt/spark/data/auc           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš™ï¸ Prerequisites

- **Docker**: 20.10+ ([Install Docker](https://docs.docker.com/get-docker/))
- **Docker Compose**: 1.29+ (included with Docker Desktop)
- **Hardware**: Minimum 8GB RAM, 20GB disk space
- **OS**: Linux, macOS, or Windows with WSL2

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/JadesonBruno/local-datalake-with-hdfs-and-spark.git
cd local-datalake-with-hdfs-and-spark
```

### 2. Create Environment File

```bash
# Copy the example .env (if provided) or create manually
cat > .env << 'EOF'
SPARK_HOME=/opt/spark
HADOOP_HOME=/opt/hadoop
EOF
```

### 3. Start the Cluster

```bash
# Start with 3 worker nodes
docker-compose up -d --scale data-lake-worker=3
```

**Expected output:**
```
âœ… Creating network "local-data-lake_default"
âœ… Creating volume "local-data-lake_data-lake-logs"
âœ… Creating master ... done
âœ… Creating data-lake-worker_1 ... done
âœ… Creating data-lake-worker_2 ... done
âœ… Creating data-lake-worker_3 ... done
âœ… Creating history-server ... done
```

### 4. Verify Cluster Health

```bash
# Check running containers
docker-compose ps

# Check HDFS status
docker exec master hdfs dfsadmin -report

# Check YARN nodes
docker exec master yarn node -list

# View logs
docker-compose logs -f master
```

## ğŸŒ Web Interfaces

Access these URLs in your browser:

| Service | URL | Description |
|---------|-----|-------------|
| **Spark Master UI** | [http://localhost:9091](http://localhost:9091) | Monitor active Spark jobs & workers |
| **HDFS NameNode UI** | [http://localhost:9871](http://localhost:9871) | Browse HDFS filesystem & health |
| **YARN ResourceManager** | [http://localhost:8081](http://localhost:8081) | Track YARN applications & resources |
| **Spark History Server** | [http://localhost:18081](http://localhost:18081) | View completed Spark job history |

## ğŸ¤– Running the ML Job

### Submit the Training Job

```bash
docker exec master spark-submit \
  --master yarn \
  --deploy-mode cluster \
  ./apps/job.py
```

### Monitor Job Progress

1. **YARN UI**: Check application status at [http://localhost:8081](http://localhost:8081)
2. **Logs**: Follow real-time logs
   ```bash
   docker-compose logs -f master
   ```
3. **History Server**: After completion, view detailed metrics at [http://localhost:18081](http://localhost:18081)

### Job Execution Flow

```
Client (You)
     â”‚
     â”œâ”€[1]â”€> Submit job to YARN
     â”‚
     v
YARN ResourceManager (Master)
     â”‚
     â”œâ”€[2]â”€> Allocate Application Master
     â”‚
     v
Spark Driver (Worker Node)
     â”‚
     â”œâ”€[3]â”€> Request executors from YARN
     â”‚
     v
Spark Executors (Worker Nodes)
     â”‚
     â”œâ”€[4]â”€> Read dataset.csv from HDFS
     â”œâ”€[5]â”€> Execute ML pipeline (transform, train, validate)
     â”œâ”€[6]â”€> Save model to HDFS
     â””â”€[7]â”€> Save AUC metric to HDFS
```

## ğŸ“Š Model Results

### Performance Metrics

After running the job, check the model performance:

```bash
# View AUC metric
docker exec master hdfs dfs -cat /opt/spark/data/auc/part-*.csv
```

**Cross-Validation Results** (from metadata):
- **Best Model**: regParam = 0.01
- **Average Metrics**:
  - regParam=0.1 â†’ AUC: **0.8951**
  - regParam=0.01 â†’ AUC: **0.9037** âœ… (Best)

### Model Artifacts

```bash
# List saved model structure
docker exec master hdfs dfs -ls -R /opt/spark/data/model
```

**Output structure:**
```
/opt/spark/data/model/
â”œâ”€â”€ bestModel/               # Winning model from cross-validation
â”‚   â”œâ”€â”€ data/
â”‚   â””â”€â”€ metadata/
â”œâ”€â”€ estimator/              # LogisticRegression configuration
â”‚   â””â”€â”€ metadata/
â”œâ”€â”€ evaluator/              # BinaryClassificationEvaluator config
â”‚   â””â”€â”€ metadata/
â””â”€â”€ metadata/               # CrossValidator params & metrics
    â””â”€â”€ part-00000          # Contains avgMetrics: [0.8951, 0.9037]
```

### Interpretation

- **AUC = 0.9037**: Excellent discrimination between income classes
- **Model Choice**: Logistic Regression with L2 regularization (regParam=0.01)
- **5-Fold CV**: Robust validation prevents overfitting

## ğŸ’¾ HDFS Operations

### Basic Commands

```bash
# List HDFS contents
docker exec master hdfs dfs -ls /opt/spark/data

# Create directory
docker exec master hdfs dfs -mkdir /opt/spark/data/test

# Copy file from local to HDFS
docker exec master hdfs dfs -put ./data/dataset.csv /opt/spark/data/

# Download file from HDFS to local
docker exec master hdfs dfs -get /opt/spark/data/model ./local_model

# View file content
docker exec master hdfs dfs -cat /opt/spark/data/auc/part-*.csv

# Check HDFS usage
docker exec master hdfs dfs -df -h

# Remove directory
docker exec master hdfs dfs -rm -r /opt/spark/data/test
```

### Data Flow Example

```bash
# 1. Upload new dataset
docker cp new_data.csv master:/opt/spark/data/
docker exec master hdfs dfs -put /opt/spark/data/new_data.csv /opt/spark/data/

# 2. Process with Spark
docker exec master spark-submit --master yarn --deploy-mode cluster ./apps/job.py

# 3. Retrieve results
docker exec master hdfs dfs -get /opt/spark/data/predictions ./results/
```

## ğŸ› ï¸ Cluster Management

### Scale Workers

```bash
# Scale to 5 workers
docker-compose up -d --scale data-lake-worker=5

# Scale down to 1 worker
docker-compose up -d --scale data-lake-worker=1
```

### Restart Services

```bash
# Restart all containers
docker-compose restart

# Restart specific service
docker-compose restart master
```

### Stop Cluster

```bash
# Stop containers (preserve data)
docker-compose stop

# Stop and remove containers (preserve volumes)
docker-compose down

# Stop and remove everything (including data)
docker-compose down --volumes --remove-orphans
```

### Rebuild After Changes

```bash
# Rebuild images without cache
docker-compose build --no-cache

# Recreate containers with new image
docker-compose up -d --force-recreate
```

## ğŸ› Troubleshooting

### Common Issues

#### 1. NameNode Not Starting

**Symptoms**: Master container exits or HDFS commands fail

**Solution**:
```bash
# Check if namenode needs formatting
docker exec master hdfs dfsadmin -report

# Force reformat (WARNING: deletes data)
docker exec master hdfs namenode -format -force
docker-compose restart master
```

#### 2. HDFS in Safe Mode

**Symptoms**: "Cannot create file... Name node is in safe mode"

**Solution**:
```bash
# Check safe mode status
docker exec master hdfs dfsadmin -safemode get

# Force exit safe mode (after verifying DataNodes are up)
docker exec master hdfs dfsadmin -safemode leave
```

#### 3. Workers Not Connecting

**Symptoms**: No DataNodes or NodeManagers visible in UIs

**Solution**:
```bash
# Check worker logs
docker-compose logs data-lake-worker

# Verify network connectivity
docker exec master ping data-lake-worker

# Restart workers
docker-compose restart data-lake-worker
```

#### 4. Port Conflicts

**Symptoms**: "Bind for 0.0.0.0:9091 failed: port is already allocated"

**Solution**:
```bash
# Find process using port
netstat -ano | findstr :9091   # Windows
lsof -i :9091                  # Linux/Mac

# Kill process or change port in docker-compose.yml
```

### Logs Analysis

```bash
# View all logs
docker-compose logs

# Follow specific service
docker-compose logs -f master

# Last 100 lines
docker-compose logs --tail=100 master

# Save logs to file
docker-compose logs > cluster_logs.txt
```

### Health Checks

```bash
# Check Java processes in master
docker exec master jps

# Expected output:
# NameNode
# SecondaryNameNode
# ResourceManager
# Jps

# Check Java processes in worker
docker exec data-lake-worker jps

# Expected output:
# DataNode
# NodeManager
# Jps
```

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feat/amazing-feature`)
3. Commit with semantic commits (`git commit -m 'âœ¨ feat: add amazing feature'`)
4. Push to branch (`git push origin feat/amazing-feature`)
5. Open a Pull Request

### Commit Convention

This project follows [Conventional Commits](https://www.conventionalcommits.org/):

- `ğŸ‰ init:` Initial commit
- `âœ¨ feat:` New feature
- `ğŸ› fix:` Bug fix
- `ğŸ“š docs:` Documentation changes
- `ğŸ§± ci:` CI/CD changes
- `â™»ï¸ refactor:` Code refactoring
- `âš¡ perf:` Performance improvements
- `ğŸ§ª test:` Tests
- `ğŸ”§ chore:` Maintenance tasks

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Apache Spark](https://spark.apache.org/) - Unified analytics engine
- [Apache Hadoop](https://hadoop.apache.org/) - Distributed storage and processing
- [Docker](https://www.docker.com/) - Containerization platform
- [UCI ML Repository](https://archive.ics.uci.edu/ml/) - Dataset source

## ğŸ“ Support and Contact

**Jadeson Bruno**
- ğŸ“§ Email: jadesonbruno.a@outlook.com
- ğŸ™ GitHub: [@JadesonBruno](https://github.com/JadesonBruno)
- ğŸ’¼ LinkedIn: [Jadeson Bruno](https://www.linkedin.com/in/jadeson-silva/)

---

â­ **If this project was helpful, please give it a star on GitHub!**

ğŸ“ **License**: MIT - see the [LICENSE](LICENSE) file for details.

**Made with â¤ï¸ by [Jadeson Bruno](https://github.com/JadesonBruno)**
